{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efab88d9-88ee-4f22-9d8d-537559b1562f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6fdbe7-dded-4dcc-833f-ac7dc74cb52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 케라스 오토인코더 로드\n",
    "autoencoder = load_model(\"autoencoder.h5\")\n",
    "print(\"autoencoder 로드 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f77c16-6a00-4048-a3c4-ab5bf9ac05e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) HMM 모델들 로드\n",
    "gesture_models = {}\n",
    "save_dir = \"saved_hmm_models\"\n",
    "for file_name in os.listdir(save_dir):\n",
    "    if file_name.endswith(\".pkl\"):\n",
    "        gesture_name = file_name.replace(\"hmm_\", \"\").replace(\".pkl\", \"\")\n",
    "        model_path = os.path.join(save_dir, file_name)\n",
    "        hmm_model = joblib.load(model_path)\n",
    "        gesture_models[gesture_name] = hmm_model\n",
    "print(f\"HMM 모델 {len(gesture_models)}개 로드 완료:\", list(gesture_models.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037ce313-f4d1-466d-8dbb-1d16f916e176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Scaler 로드 (훈련 때 사용한 scaler.pkl 가 있다고 가정)\n",
    "scaler = joblib.load(\"scaler.pkl\")\n",
    "print(\"Scaler 로드 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2226d6-e2d4-478d-81d2-82bdb04b459f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Mediapipe 세팅\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.7,\n",
    "    min_tracking_confidence=0.7\n",
    ")\n",
    "\n",
    "encoder = load_model(\"encoder.h5\")  # encoder만 로드했다고 가정\n",
    "print(\"encoder 로드 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f094d94-0d0d-4839-a7c0-ce9fd5125b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_hmm(latent_seq_2d, gesture_models):\n",
    "    \"\"\"\n",
    "    latent_seq_2d: shape=(T, latent_dim)\n",
    "    gesture_models: { gesture_name: hmm_model, ... }\n",
    "    return: (best_label, best_score)\n",
    "    \"\"\"\n",
    "    best_label = None\n",
    "    best_score = -np.inf\n",
    "    length = [latent_seq_2d.shape[0]]\n",
    "    for gesture_name, model in gesture_models.items():\n",
    "        score = model.score(latent_seq_2d, length)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_label = gesture_name\n",
    "    return best_label, best_score\n",
    "\n",
    "def get_landmark_63(hand_landmarks):\n",
    "    \"\"\"\n",
    "    Mediapipe의 hand_landmarks(21개)에 대해 (x,y,z) 63차원 리스트 반환\n",
    "    \"\"\"\n",
    "    row_data = []\n",
    "    for lm in hand_landmarks.landmark:  # 21개\n",
    "        row_data.extend([lm.x, lm.y, lm.z])\n",
    "    return row_data\n",
    "\n",
    "import time\n",
    "\n",
    "# 웹캠 열기\n",
    "cap = cv2.VideoCapture(0)  # 0번 카메라\n",
    "\n",
    "FRAME_WINDOW = 60  # 누적 프레임 수 (예시: 60프레임=약2초)\n",
    "sequence_data = []\n",
    "\n",
    "while True:\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        print(\"카메라에서 영상을 받지 못했습니다.\")\n",
    "        break\n",
    "\n",
    "    # 이미지 좌우 반전(편의상)\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Mediapipe 입력을 위해 BGR->RGB 변환\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(image_rgb)\n",
    "\n",
    "    # 현재 프레임의 (x,y,z) 63차원 추출\n",
    "    if results.multi_hand_landmarks:\n",
    "        # 하나의 손만 사용한다고 가정\n",
    "        hand_landmarks = results.multi_hand_landmarks[0]\n",
    "        row_data = get_landmark_63(hand_landmarks)\n",
    "    else:\n",
    "        # 손이 검출 안 되었으면 63차원 0\n",
    "        row_data = [0.0]*63\n",
    "\n",
    "    sequence_data.append(row_data)\n",
    "\n",
    "    # 일정 길이 쌓이면 한 번 분류\n",
    "    if len(sequence_data) >= FRAME_WINDOW:\n",
    "        # numpy 변환 (T, 63)\n",
    "        seq_array = np.array(sequence_data)\n",
    "\n",
    "        # 1) Scaler 적용 -> (T, 63)\n",
    "        seq_scaled = scaler.transform(seq_array)\n",
    "\n",
    "        # 2) 차원 맞추기 => (1, T, 63)\n",
    "        seq_scaled = np.expand_dims(seq_scaled, axis=0)\n",
    "\n",
    "        # 3) 인코더 통과 => (1, T, latent_dim)\n",
    "        latent_seq = encoder.predict(seq_scaled)  \n",
    "        # latent_seq.shape = (1, T, latent_dim)\n",
    "\n",
    "        # hmm은 (T, latent_dim) 필요\n",
    "        latent_seq_2d = latent_seq[0]\n",
    "\n",
    "        # 4) HMM 분류\n",
    "        pred_label, pred_score = classify_hmm(latent_seq_2d, gesture_models)\n",
    "        print(f\"[HMM] Predicted gesture: {pred_label} (score={pred_score:.2f})\")\n",
    "\n",
    "        # 결과 표시(화면에 텍스트로)\n",
    "        cv2.putText(frame, f\"Pred: {pred_label}\", (50,50),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2, cv2.LINE_AA)\n",
    "\n",
    "        # 다음번 인식을 위해 sequence_data 초기화\n",
    "        # (혹은 슬라이딩 윈도우로 일부 프레임만 유지하도록 설계 가능)\n",
    "        sequence_data = []\n",
    "\n",
    "    # 디스플레이\n",
    "    cv2.imshow(\"Real-time Hand Gesture\", frame)\n",
    "\n",
    "    # ESC 키(27) 누르면 종료\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7e4c92-25d0-4eb8-b59b-c110beffb086",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0fcab9-e73e-47f3-8006-426a4636fc7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_new_env)",
   "language": "python",
   "name": "my_new_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
